# Day 15: JSON Log Processing System with Schema Validation

A production-ready distributed log processing system demonstrating JSON schema validation, message queuing, caching strategies, and observability patterns at scale.

## üèóÔ∏è System Architecture

### Core Components

1. **API Gateway** (Port 8080)
   - Spring Cloud Gateway for routing
   - Redis-backed rate limiting (1000 req/s per IP)
   - Circuit breaker with fallback handling
   - Distributed tracing

2. **Log Producer** (Port 8081)
   - JSON Schema validation engine
   - Kafka producer with idempotent writes
   - Three-tier caching (In-memory ‚Üí Redis ‚Üí PostgreSQL)
   - Dead Letter Queue for invalid messages
   - Circuit breaker for schema validation

3. **Log Consumer** (Port 8082)
   - Kafka consumer with manual acknowledgment
   - Batch processing and transaction management
   - PostgreSQL persistence with JSONB support
   - Real-time statistics aggregation

### Infrastructure

- **Apache Kafka**: Message streaming (Topics: `logs`, `logs.dlq`)
- **Redis**: Schema caching and rate limiting
- **PostgreSQL**: Log persistence with indexed queries
- **Prometheus**: Metrics collection
- **Grafana**: Visualization dashboards

## üöÄ Quick Start

### Prerequisites

- Docker 20.10+
- Docker Compose 2.0+
- 8GB RAM minimum
- Ports available: 8080-8082, 3000, 5432, 6379, 9090, 9092

### Setup (3 minutes)

```bash
# 1. Generate the system (if using the generator script)
chmod +x generate_system_files.sh
./generate_system_files.sh

# 2. Start all services
cd json-log-processor
./setup.sh

# 3. Verify system health
curl http://localhost:8080/api/logs/health
```

The setup script will:
- Build all Docker images
- Start infrastructure (Kafka, Redis, PostgreSQL)
- Deploy microservices
- Wait for health checks
- Display service URLs

## üìä Usage Examples

### Single Log Ingestion

```bash
curl -X POST http://localhost:8080/api/logs/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "level": "INFO",
    "timestamp": "2025-01-15T10:30:00.000Z",
    "message": "User login successful",
    "service": "auth-service",
    "metadata": {
      "userId": "12345",
      "ipAddress": "192.168.1.1"
    },
    "traceId": "a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6",
    "spanId": "a1b2c3d4e5f6g7h8"
  }'
```

**Response (202 Accepted):**
```json
{
  "status": "accepted",
  "message": "Log event accepted for processing"
}
```

### Batch Ingestion

```bash
curl -X POST http://localhost:8080/api/logs/ingest/batch \
  -H "Content-Type: application/json" \
  -d '[
    {
      "level": "ERROR",
      "timestamp": "2025-01-15T10:31:00.000Z",
      "message": "Database connection failed",
      "service": "payment-service"
    },
    {
      "level": "WARN",
      "timestamp": "2025-01-15T10:31:05.000Z",
      "message": "High memory usage detected",
      "service": "order-service"
    }
  ]'
```

**Response (200 OK):**
```json
{
  "total": 2,
  "accepted": 2,
  "rejected": 0
}
```

### Schema Validation Example

Invalid log (missing required field):

```bash
curl -X POST http://localhost:8080/api/logs/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "level": "INFO",
    "message": "Missing timestamp field"
  }'
```

**Response (422 Unprocessable Entity):**
```json
{
  "status": "rejected",
  "message": "Log event failed schema validation",
  "error": "timestamp: is required; service: is required"
}
```

## üß™ Testing

### Integration Tests

```bash
# Run full integration test suite
./integration-tests/test_system.sh
```

Tests include:
- Health check validation
- Valid log ingestion
- Schema validation errors
- Invalid log level handling
- Batch processing
- High-volume testing (100 logs)

### Load Testing

```bash
# Run load test (100 req/s for 60 seconds)
./load-test.sh
```

Generates:
- 6,000 total requests
- 10 concurrent connections
- Results in `/tmp/load_test_results.tsv`

Expected performance:
- **Throughput**: 50K+ logs/second per instance
- **P99 Latency**: <10ms for validation
- **Error Rate**: <0.1% under normal load

## üìà Monitoring

### Prometheus Metrics

Access Prometheus at http://localhost:9090

Key metrics:
```promql
# Validation success rate
rate(kafka_producer_success_total[1m])

# Validation failures
rate(kafka_producer_failure_total[1m])

# DLQ depth
kafka_producer_dlq_total

# Processing latency
histogram_quantile(0.99, rate(kafka_producer_send_duration_bucket[1m]))

# Consumer throughput
rate(kafka_consumer_processed_total[1m])
```

### Grafana Dashboards

Access Grafana at http://localhost:3000 (admin/admin)

Pre-configured dashboards:
1. **System Overview**: Overall health and throughput
2. **Validation Metrics**: Schema validation success/failure rates
3. **Kafka Metrics**: Topic lag, consumer group status
4. **Infrastructure**: Redis cache hit rate, PostgreSQL connections

### Log Inspection

```bash
# View producer logs
docker compose logs -f log-producer

# View consumer logs
docker compose logs -f log-consumer

# View all services
docker compose logs -f
```

## üîç Validation Pipeline Details

### Schema Validation Stages

1. **Syntactic Validation** (100¬µs avg)
   - Parse JSON structure
   - Check field types
   - Verify required fields

2. **Schema Conformance** (500¬µs avg with cache)
   - Load schema from cache (Redis L2)
   - Validate against JSON Schema v7
   - Check enum constraints and patterns

3. **Business Rules** (200¬µs avg)
   - Timestamp not in future
   - Message length within limits
   - Service name format validation

### Caching Strategy

**Three-Tier Pyramid:**

1. **L1: In-Memory LRU** (1¬µs lookup)
   - 10K schemas max
   - 99.5% hit rate after warm-up

2. **L2: Redis Cache** (2ms lookup)
   - 1M schemas max
   - 1-hour TTL
   - Handles cross-instance misses

3. **L3: Schema Registry** (50ms lookup)
   - PostgreSQL-backed
   - Version control
   - Cold storage

### Dead Letter Queue Processing

Failed validations are routed to `logs.dlq` topic with:
- Original event payload
- Validation error details
- Timestamp of failure
- Retry metadata

DLQ consumer implements exponential backoff:
- Initial retry: 1s delay
- Max delay: 5 minutes
- Max retries: 10
- Manual review after exhaustion

## üèõÔ∏è System Design Patterns

### 1. Producer-Side Validation (Fail-Fast)

**Why**: Catch errors at the source before wasting network and storage resources.

**Trade-off**: Adds latency to producer requests vs. accepting everything and validating later.

**Our Choice**: Validate synchronously with <1ms P99 latency. The cost is negligible compared to processing invalid data downstream.

### 2. Schema Registry with Distributed Caching

**Why**: Schema fetches are expensive (50ms from DB) and happen on every validation.

**Trade-off**: Cached schemas may be stale vs. always fetching latest version.

**Our Choice**: Cache with content-based hashing. Schema changes invalidate cache automatically via new hash. Best of both worlds.

### 3. Dead Letter Queue with Bounded Retry

**Why**: Transient failures (schema registry down) shouldn't lose data, but permanent failures shouldn't block processing.

**Trade-off**: Unlimited retries cause memory leaks vs. dropping messages loses data.

**Our Choice**: Exponential backoff with 10-retry limit, then manual review. Balances recovery with operational safety.

### 4. Circuit Breaker on Schema Validation

**Why**: If validation consistently fails (e.g., schema registry down), don't DOS the registry with retries.

**Trade-off**: Accepting unvalidated data vs. dropping everything during outages.

**Our Choice**: Circuit breaker with fallback that logs warning but accepts events. Availability > strict validation during outages.

### 5. Idempotent Kafka Producer

**Why**: Network failures can cause duplicate sends. Idempotence guarantees exactly-once semantics.

**Trade-off**: Increased latency (15ms vs. 2ms for acks=all) vs. potential duplicates.

**Our Choice**: Durability is critical for logging. We accept the latency trade-off for guaranteed delivery.

## üö® Failure Scenarios

### Scenario 1: Schema Registry Unavailable

**Impact:**
- Validation fails for new schemas
- Existing schemas serve from Redis cache
- System continues operating with cached schemas

**Recovery:**
- Circuit breaker opens after 10 failures
- Fallback accepts events with warning
- Automatic recovery when registry returns

**Time to Recover:** <30 seconds

### Scenario 2: Redis Cache Flush

**Impact:**
- Cache miss rate spikes to 100%
- Validation latency increases to 50ms (DB fetch)
- First 1000 requests experience slowdown

**Recovery:**
- Cache rebuilds automatically from DB
- LRU eviction handles hot schemas
- Normal performance resumes

**Time to Recover:** 30-60 seconds

### Scenario 3: Kafka Broker Failure

**Impact:**
- Producer retries fail
- Events queue in memory (bounded)
- After 10 retries, events rejected

**Recovery:**
- Kafka failover to replica
- Producer reconnects automatically
- Queued events drain on reconnection

**Time to Recover:** 2-5 minutes (Kafka failover time)

### Scenario 4: PostgreSQL Overload

**Impact:**
- Consumer processing slows
- Kafka consumer lag increases
- No data loss (Kafka retains messages)

**Recovery:**
- Consumer batch size reduced
- Connection pool scales
- Lag drains once DB recovers

**Time to Recover:** Varies by lag depth

## üéØ Scale Connection: Enterprise Patterns

This system demonstrates patterns used by:

**Netflix** (500B events/day):
- Schema validation at ingestion (our Pattern #1)
- Three-tier caching for schemas (our Pattern #2)
- Dead letter queues for resilience (our Pattern #3)

**Uber** (2,500 active schemas):
- Schema-as-code with version control
- Enforced compatibility rules
- Zero-downtime schema evolution

**Amazon S3** (0.001% error rate):
- Multi-stage validation pipeline
- Circuit breakers at each layer
- Comprehensive observability

## üìö Key Learnings

1. **Validate Early**: Producer-side validation saves 85% of wasted resources
2. **Cache Aggressively**: Three-tier caching reduces validation latency 30x
3. **Fail Gracefully**: Circuit breakers prevent cascading failures
4. **Monitor Everything**: You can't improve what you don't measure
5. **Plan for Failure**: Dead letter queues are not optional at scale

## üîß Configuration

### Tuning for Different Workloads

**High Throughput (>100K msg/s)**:
- Increase Kafka `batch.size` to 32KB
- Set `linger.ms` to 20ms
- Scale consumers horizontally (3+ instances)

**Low Latency (<1ms P99)**:
- Set `linger.ms` to 0
- Reduce `batch.size` to 4KB
- Use SSD-backed PostgreSQL

**Cost Optimization**:
- Increase `linger.ms` to reduce requests
- Enable compression (`gzip`)
- Use tiered storage for old logs

## üßπ Cleanup

```bash
# Stop all services
docker compose down

# Remove volumes (deletes all data)
docker compose down -v

# Remove images
docker compose down --rmi all
```

## üìñ Next Steps

### Day 16 Preview: Protocol Buffers

Tomorrow you'll:
- Replace JSON with Protocol Buffers
- Achieve 5x throughput improvement
- Reduce message size by 10x
- Compare schema-first vs. code-first approaches

**Hint**: The validation patterns you built today work identically with Protobuf. Only serialization format changes.

## üÜò Troubleshooting

### Services Won't Start

```bash
# Check Docker resources
docker system df

# View service logs
docker compose logs [service-name]

# Restart specific service
docker compose restart [service-name]
```

### High Kafka Lag

```bash
# Check consumer lag
docker exec -it kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 \
  --describe --group log-consumer-group
```

### Redis Connection Issues

```bash
# Test Redis connectivity
docker exec -it redis redis-cli ping

# Check cache stats
docker exec -it redis redis-cli info stats
```

## üìÑ License

Educational project for Day 15 of 254-Day System Design Course.

---

**Built with ‚ù§Ô∏è using Spring Boot, Apache Kafka, and production-grade distributed system patterns.**
